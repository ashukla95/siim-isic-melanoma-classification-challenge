{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.image as img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train_df[list(train_df.keys().drop(\"image_id\"))]\n",
    "img_size = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"images/\"\n",
    "train_image = []\n",
    "for name in train_df[\"image_id\"]:\n",
    "    image = Image.open(image_path+name+\".jpg\")\n",
    "    image = image.resize((img_size, img_size))\n",
    "    train_image.append(image)\n",
    "x_train = np.ndarray(shape=(len(train_image), img_size, img_size, 3), dtype=np.float32)\n",
    "for i in range(len(train_image)):\n",
    "    x_train[i] = img_to_array(train_image[i])\n",
    "x_train /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1821, 400, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df.loc[:, \"healthy\":].values\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.15, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1547, 400, 400, 3), (274, 400, 400, 3), (1547, 4), (274, 4))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=69)\n",
    "x_train, y_train = smote.fit_resample(x_train.reshape((-1, img_size*img_size*3)), y_train)\n",
    "x_train = x_train.reshape((-1, img_size, img_size, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2128, 400, 400, 3), array([532, 532, 532, 532]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_accuracy\",\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=0.00001,\n",
    "    verbose=1\n",
    ")\n",
    "es = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential, load_model, Input\n",
    "from keras.layers import (Conv2D, Dense, MaxPooling2D, LeakyReLU, Reshape,\n",
    "                          Flatten, Dropout, BatchNormalization)\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 400, 400, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 406, 406, 3)  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 200, 200, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 200, 200, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 200, 200, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 202, 202, 64) 0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 100, 100, 64) 0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 100, 100, 64) 4160        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 100, 100, 64) 256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 100, 100, 64) 0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 100, 100, 64) 36928       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 100, 100, 64) 256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 100, 100, 64) 0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 100, 100, 256 16640       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 100, 100, 256 16640       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 100, 100, 256 1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 100, 100, 256 1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 100, 100, 256 0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 100, 100, 256 0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 100, 100, 64) 16448       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 100, 100, 64) 256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 100, 100, 64) 0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 100, 100, 64) 36928       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 100, 100, 64) 256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 100, 100, 64) 0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 100, 100, 256 16640       activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 100, 100, 256 1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 100, 100, 256 0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 100, 100, 256 0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 100, 100, 64) 16448       activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 100, 100, 64) 256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 100, 100, 64) 0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 100, 100, 64) 36928       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 100, 100, 64) 256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 100, 100, 64) 0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 100, 100, 256 16640       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 100, 100, 256 1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 100, 100, 256 0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 100, 100, 256 0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 50, 50, 128)  32896       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 50, 50, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 50, 50, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 50, 50, 128)  147584      activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 50, 50, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 50, 50, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 50, 50, 512)  66048       activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 50, 50, 512)  131584      activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 50, 50, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 50, 50, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 50, 50, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 50, 50, 512)  0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 50, 50, 128)  65664       activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 50, 50, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 50, 50, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 50, 50, 128)  147584      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 50, 50, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 50, 50, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 50, 50, 512)  66048       activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 50, 50, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 50, 50, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 50, 50, 512)  0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 50, 50, 128)  65664       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 50, 50, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 50, 50, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 50, 50, 128)  147584      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 50, 50, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 50, 50, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 50, 50, 512)  66048       activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 50, 50, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 50, 50, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 50, 50, 512)  0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 50, 50, 128)  65664       activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 50, 50, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 50, 50, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 50, 50, 128)  147584      activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 50, 50, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 50, 50, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 50, 50, 512)  66048       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 50, 50, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 50, 50, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 50, 50, 512)  0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 25, 25, 256)  131328      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 25, 25, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 25, 25, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 25, 25, 256)  590080      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 25, 25, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 25, 25, 256)  0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 25, 25, 1024) 263168      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 25, 25, 1024) 525312      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 25, 25, 1024) 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 25, 25, 1024) 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 25, 25, 1024) 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 25, 25, 1024) 0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 25, 25, 256)  262400      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 25, 25, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 25, 25, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 25, 25, 256)  590080      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 25, 25, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 25, 25, 256)  0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 25, 25, 1024) 263168      activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 25, 25, 1024) 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 25, 25, 1024) 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 25, 25, 1024) 0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 25, 25, 256)  262400      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 25, 25, 256)  1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 25, 25, 256)  0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 25, 25, 256)  590080      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 25, 25, 256)  1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 25, 25, 256)  0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 25, 25, 1024) 263168      activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 25, 25, 1024) 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 25, 25, 1024) 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 25, 25, 1024) 0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 25, 25, 256)  262400      activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 25, 25, 256)  1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 25, 25, 256)  0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 25, 25, 256)  590080      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 25, 25, 256)  1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 25, 25, 256)  0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 25, 25, 1024) 263168      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 25, 25, 1024) 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 25, 25, 1024) 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 25, 25, 1024) 0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 25, 25, 256)  262400      activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 25, 25, 256)  1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 25, 25, 256)  0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 25, 25, 256)  590080      activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 25, 25, 256)  1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 25, 25, 256)  0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 25, 25, 1024) 263168      activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 25, 25, 1024) 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 25, 25, 1024) 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 25, 25, 1024) 0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 25, 25, 256)  262400      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 25, 25, 256)  1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 25, 25, 256)  0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 25, 25, 256)  590080      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 25, 25, 256)  1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 25, 25, 256)  0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 25, 25, 1024) 263168      activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 25, 25, 1024) 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 25, 25, 1024) 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 25, 25, 1024) 0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 13, 13, 512)  524800      activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 13, 13, 512)  2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 13, 13, 512)  0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 13, 13, 512)  2359808     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 13, 13, 512)  2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 13, 13, 512)  0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 13, 13, 2048) 1050624     activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 13, 13, 2048) 2099200     activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 13, 13, 2048) 8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 13, 13, 2048) 8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 13, 13, 2048) 0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 13, 13, 2048) 0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 13, 13, 512)  1049088     activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 13, 13, 512)  2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 13, 13, 512)  0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 13, 13, 512)  2359808     activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 13, 13, 512)  2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 13, 13, 512)  0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 13, 13, 2048) 1050624     activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 13, 13, 2048) 8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 13, 13, 2048) 0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 13, 13, 2048) 0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 13, 13, 512)  1049088     activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 13, 13, 512)  2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 13, 13, 512)  0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 13, 13, 512)  2359808     activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 13, 13, 512)  2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 13, 13, 512)  0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 13, 13, 2048) 1050624     activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 13, 13, 2048) 8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 13, 13, 2048) 0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 13, 13, 2048) 0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 11, 11, 32)   589856      activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 11, 11, 32)   96          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 9, 9, 32)     9248        batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 9, 9, 32)     96          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 5, 5, 32)     0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 5, 5, 32)     0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 3, 3, 64)     18496       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 3, 3, 64)     192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 1, 1, 64)     36928       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 1, 1, 64)     192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 1, 1, 64)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1, 1, 64)     0           max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 1, 1, 128)    8320        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 1, 1, 128)    384         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 1, 1, 128)    16512       batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 1, 1, 128)    384         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 1, 1, 128)    0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1, 1, 128)    0           max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 128)          0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          33024       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 256)          768         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 256)          0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           16448       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 64)           192         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            260         batch_normalization_12[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 24,319,108\n",
      "Trainable params: 24,264,452\n",
      "Non-trainable params: 54,656\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import ResNet50\n",
    "reg=0.0005\n",
    "\n",
    "\n",
    "model_d121 = ResNet50(include_top=False, weights=\"imagenet\",\n",
    "                           input_shape=(img_size, img_size, 3))\n",
    "model_d121.trainable = False\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(model_i)\n",
    "# filter 32\n",
    "filter=32\n",
    "model = Conv2D(filter, kernel_size=(3,3), data_format=\"channels_last\" ,activation=\"relu\", kernel_regularizer=l2(reg))(model_d121.output)\n",
    "model = BatchNormalization(axis=-1, center=True, scale=False)(model)\n",
    "model = Conv2D(filter, kernel_size=(3,3), activation=\"relu\", kernel_regularizer=l2(reg))(model)\n",
    "model = BatchNormalization(axis=-1, center=True, scale=False)(model)\n",
    "model = MaxPooling2D(pool_size=(2,2), padding=\"SAME\")(model)\n",
    "model = Dropout(0.25)(model)\n",
    "\n",
    "# # filter 64\n",
    "filter=64\n",
    "model = Conv2D(filter, kernel_size=(3,3), data_format=\"channels_last\" ,activation=\"relu\", kernel_regularizer=l2(reg))(model)\n",
    "model = BatchNormalization(axis=-1, center=True, scale=False)(model)\n",
    "model = Conv2D(filter, kernel_size=(3,3), activation=\"relu\", kernel_regularizer=l2(reg))(model)\n",
    "model = BatchNormalization(axis=-1, center=True, scale=False)(model)\n",
    "model = MaxPooling2D(pool_size=(2,2), padding=\"SAME\")(model)\n",
    "model = Dropout(0.25)(model)\n",
    "\n",
    "# # #filter 128\n",
    "# # filter = 128\n",
    "filter=128\n",
    "model = Conv2D(filter, kernel_size=(1,1), data_format=\"channels_last\" ,activation=\"relu\", input_shape=(img_size,img_size,3), kernel_regularizer=l2(reg))(model)\n",
    "model = BatchNormalization(axis=-1, center=True, scale=False)(model)\n",
    "model = Conv2D(filter, kernel_size=(1,1), activation=\"relu\", kernel_regularizer=l2(reg))(model)\n",
    "model = BatchNormalization(axis=-1, center=True, scale=False)(model)\n",
    "model = MaxPooling2D(pool_size=(2,2), padding=\"SAME\")(model)\n",
    "model = Dropout(0.25)(model)\n",
    "\n",
    "# # filter = 256\n",
    "# filter=256\n",
    "# model = Conv2D(filter, kernel_size=(3,3), data_format=\"channels_last\" ,activation=\"relu\", input_shape=(img_size,img_size,3), kernel_regularizer=l2(reg))(model)\n",
    "# model = BatchNormalization(axis=-1, center=True, scale=False)(model)\n",
    "# model = Conv2D(filter, kernel_size=(3,3), activation=\"relu\", kernel_regularizer=l2(reg))(model)\n",
    "# model = BatchNormalization(axis=-1, center=True, scale=False)(model)\n",
    "# model = MaxPooling2D(pool_size=(2,2), padding=\"SAME\")(model)\n",
    "# model = Dropout(0.25)(model)\n",
    "\n",
    "\n",
    "# # filter = 512\n",
    "# filter=512\n",
    "# model = Conv2D(filter, kernel_size=(3,3), data_format=\"channels_last\" ,activation=\"relu\", input_shape=(img_size,img_size,3), kernel_regularizer=l2(reg))(model)\n",
    "# model = BatchNormalization(axis=-1, center=True, scale=False)(model)\n",
    "# model = Conv2D(filter, kernel_size=(3,3), activation=\"relu\", kernel_regularizer=l2(reg))(model)\n",
    "# model = BatchNormalization(axis=-1, center=True, scale=False)(model)\n",
    "# model = MaxPooling2D(pool_size=(2,2), padding=\"SAME\")(model)\n",
    "# model = Dropout(0.25)(model)\n",
    "\n",
    "model = Flatten()(model)\n",
    "model = Dense(256, activation=\"relu\")(model)\n",
    "model = BatchNormalization(axis=-1, center=True, scale=False)(model)\n",
    "model = Dropout(0.5)(model)\n",
    "model = Dense(64, activation=\"relu\")(model)\n",
    "model = BatchNormalization(axis=-1, center=True, scale=False)(model)\n",
    "\n",
    "\n",
    "output = Dense(4, activation=\"softmax\")(model)\n",
    "model_d121 = Model(inputs=model_d121.input, outputs=output)\n",
    "model_d121.summary()\n",
    "model_d121.compile(optimizer=\"adam\", \n",
    "             loss=\"categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=45,\n",
    "    shear_range=0.25,\n",
    "    zoom_range=0.25,\n",
    "    width_shift_range=0.25,\n",
    "    height_shift_range=0.25,\n",
    "    rescale=1/255,\n",
    "    brightness_range=[0.5, 1.5],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "266/266 [==============================] - 168s 630ms/step - loss: 1.9418 - accuracy: 0.2702 - val_loss: 1.6475 - val_accuracy: 0.3285\n",
      "Epoch 2/500\n",
      "266/266 [==============================] - 170s 640ms/step - loss: 1.7361 - accuracy: 0.2758 - val_loss: 1.7870 - val_accuracy: 0.0438\n",
      "Epoch 3/500\n",
      "266/266 [==============================] - 174s 654ms/step - loss: 1.7043 - accuracy: 0.2697 - val_loss: 1.7045 - val_accuracy: 0.0438\n",
      "Epoch 4/500\n",
      "266/266 [==============================] - 174s 653ms/step - loss: 1.6719 - accuracy: 0.2721 - val_loss: 1.8255 - val_accuracy: 0.0657\n",
      "Epoch 5/500\n",
      "266/266 [==============================] - 174s 656ms/step - loss: 1.6651 - accuracy: 0.2871 - val_loss: 1.6171 - val_accuracy: 0.2007\n",
      "Epoch 6/500\n",
      "266/266 [==============================] - 174s 655ms/step - loss: 1.6426 - accuracy: 0.2787 - val_loss: 1.6788 - val_accuracy: 0.2007\n",
      "Epoch 7/500\n",
      "266/266 [==============================] - 174s 655ms/step - loss: 1.6361 - accuracy: 0.2707 - val_loss: 3.4984 - val_accuracy: 0.0693\n",
      "Epoch 8/500\n",
      "266/266 [==============================] - 174s 655ms/step - loss: 1.6290 - accuracy: 0.2735 - val_loss: 1.8514 - val_accuracy: 0.3613\n",
      "Epoch 9/500\n",
      "266/266 [==============================] - 174s 655ms/step - loss: 1.6312 - accuracy: 0.2617 - val_loss: 1.6471 - val_accuracy: 0.3212\n",
      "Epoch 10/500\n",
      "266/266 [==============================] - 174s 654ms/step - loss: 1.6245 - accuracy: 0.2599 - val_loss: 1.6019 - val_accuracy: 0.3358\n",
      "Epoch 11/500\n",
      "266/266 [==============================] - 174s 655ms/step - loss: 1.6293 - accuracy: 0.2467 - val_loss: 1.5769 - val_accuracy: 0.1752\n",
      "Epoch 12/500\n",
      "266/266 [==============================] - 174s 655ms/step - loss: 1.6096 - accuracy: 0.2627 - val_loss: 1.5830 - val_accuracy: 0.3467\n",
      "Epoch 13/500\n",
      "266/266 [==============================] - 174s 654ms/step - loss: 1.5918 - accuracy: 0.3153 - val_loss: 1.6103 - val_accuracy: 0.2044\n",
      "Epoch 14/500\n",
      "266/266 [==============================] - 174s 655ms/step - loss: 1.5742 - accuracy: 0.3158 - val_loss: 1.5553 - val_accuracy: 0.3467\n",
      "Epoch 15/500\n",
      "266/266 [==============================] - 174s 655ms/step - loss: 1.5679 - accuracy: 0.3289 - val_loss: 1.5210 - val_accuracy: 0.3431\n",
      "Epoch 16/500\n",
      "266/266 [==============================] - 174s 655ms/step - loss: 1.5488 - accuracy: 0.3492 - val_loss: 1.6022 - val_accuracy: 0.2007\n",
      "Epoch 17/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 1.5517 - accuracy: 0.3139 - val_loss: 1.4978 - val_accuracy: 0.3759\n",
      "Epoch 18/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 1.5431 - accuracy: 0.3285 - val_loss: 1.7829 - val_accuracy: 0.2810\n",
      "Epoch 19/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 1.5525 - accuracy: 0.2881 - val_loss: 1.4854 - val_accuracy: 0.3467\n",
      "Epoch 20/500\n",
      "266/266 [==============================] - 175s 656ms/step - loss: 1.5599 - accuracy: 0.2716 - val_loss: 1.5797 - val_accuracy: 0.3102\n",
      "Epoch 21/500\n",
      "266/266 [==============================] - 174s 656ms/step - loss: 1.5501 - accuracy: 0.2815 - val_loss: 1.5135 - val_accuracy: 0.2664\n",
      "Epoch 22/500\n",
      "266/266 [==============================] - 175s 656ms/step - loss: 1.5106 - accuracy: 0.3252 - val_loss: 1.5227 - val_accuracy: 0.2336\n",
      "Epoch 23/500\n",
      "266/266 [==============================] - 175s 659ms/step - loss: 1.5076 - accuracy: 0.3097 - val_loss: 1.5199 - val_accuracy: 0.3212\n",
      "Epoch 24/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 1.4983 - accuracy: 0.3181 - val_loss: 6.0443 - val_accuracy: 0.1387\n",
      "Epoch 25/500\n",
      "266/266 [==============================] - 174s 655ms/step - loss: 1.4937 - accuracy: 0.3252 - val_loss: 1.4950 - val_accuracy: 0.2591\n",
      "Epoch 26/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 1.4585 - accuracy: 0.3590 - val_loss: 1.4564 - val_accuracy: 0.4015\n",
      "Epoch 27/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 1.4627 - accuracy: 0.3459 - val_loss: 1.4504 - val_accuracy: 0.3431\n",
      "Epoch 28/500\n",
      "266/266 [==============================] - 175s 656ms/step - loss: 1.4347 - accuracy: 0.3600 - val_loss: 1.4238 - val_accuracy: 0.4124\n",
      "Epoch 29/500\n",
      "266/266 [==============================] - 175s 656ms/step - loss: 1.4183 - accuracy: 0.3618 - val_loss: 1.3791 - val_accuracy: 0.3942\n",
      "Epoch 30/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 1.4045 - accuracy: 0.3792 - val_loss: 1.3806 - val_accuracy: 0.3650\n",
      "Epoch 31/500\n",
      "266/266 [==============================] - 174s 656ms/step - loss: 1.4310 - accuracy: 0.3524 - val_loss: 6.5151 - val_accuracy: 0.1971\n",
      "Epoch 32/500\n",
      "266/266 [==============================] - 175s 656ms/step - loss: 1.4457 - accuracy: 0.3153 - val_loss: 1.3996 - val_accuracy: 0.3139\n",
      "Epoch 33/500\n",
      "266/266 [==============================] - 175s 656ms/step - loss: 1.4117 - accuracy: 0.3534 - val_loss: 1.3797 - val_accuracy: 0.3650\n",
      "Epoch 34/500\n",
      "266/266 [==============================] - 174s 656ms/step - loss: 1.3966 - accuracy: 0.3567 - val_loss: 1.3947 - val_accuracy: 0.3175\n",
      "Epoch 35/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 1.3929 - accuracy: 0.3553 - val_loss: 4.1335 - val_accuracy: 0.2591\n",
      "Epoch 36/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 1.3738 - accuracy: 0.3764 - val_loss: 1.3989 - val_accuracy: 0.2445\n",
      "Epoch 37/500\n",
      "266/266 [==============================] - 177s 666ms/step - loss: 1.3906 - accuracy: 0.3421 - val_loss: 1.3277 - val_accuracy: 0.2482\n",
      "Epoch 38/500\n",
      "266/266 [==============================] - 176s 662ms/step - loss: 1.3672 - accuracy: 0.3694 - val_loss: 20.7138 - val_accuracy: 0.1058\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 39/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 1.3570 - accuracy: 0.3736 - val_loss: 1.3375 - val_accuracy: 0.3869\n",
      "Epoch 40/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 1.3385 - accuracy: 0.3938 - val_loss: 1.3003 - val_accuracy: 0.4270\n",
      "Epoch 41/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 1.3440 - accuracy: 0.3759 - val_loss: 1.2850 - val_accuracy: 0.3905\n",
      "Epoch 42/500\n",
      "266/266 [==============================] - 175s 659ms/step - loss: 1.3444 - accuracy: 0.3759 - val_loss: 1.3383 - val_accuracy: 0.3832\n",
      "Epoch 43/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 1.3411 - accuracy: 0.3642 - val_loss: 1.2878 - val_accuracy: 0.4015\n",
      "Epoch 44/500\n",
      "266/266 [==============================] - 175s 656ms/step - loss: 1.3248 - accuracy: 0.3853 - val_loss: 1.2675 - val_accuracy: 0.4562\n",
      "Epoch 45/500\n",
      "266/266 [==============================] - 175s 656ms/step - loss: 1.3180 - accuracy: 0.3731 - val_loss: 1.2919 - val_accuracy: 0.3723\n",
      "Epoch 46/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 1.3149 - accuracy: 0.3759 - val_loss: 1.2646 - val_accuracy: 0.3832\n",
      "Epoch 47/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 1.3063 - accuracy: 0.3849 - val_loss: 1.2760 - val_accuracy: 0.4124\n",
      "Epoch 48/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 1.3172 - accuracy: 0.3755 - val_loss: 1.2621 - val_accuracy: 0.4015\n",
      "Epoch 49/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 1.3083 - accuracy: 0.3755 - val_loss: 1.2869 - val_accuracy: 0.3613\n",
      "Epoch 50/500\n",
      "266/266 [==============================] - 174s 655ms/step - loss: 1.3076 - accuracy: 0.3877 - val_loss: 1.5709 - val_accuracy: 0.4307\n",
      "Epoch 51/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 1.3091 - accuracy: 0.3910 - val_loss: 1.2643 - val_accuracy: 0.4343\n",
      "Epoch 52/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 1.3134 - accuracy: 0.3783 - val_loss: 1.2372 - val_accuracy: 0.4015\n",
      "Epoch 53/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 1.2722 - accuracy: 0.4337 - val_loss: 1.1567 - val_accuracy: 0.5730\n",
      "Epoch 54/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 1.2441 - accuracy: 0.4521 - val_loss: 1.0798 - val_accuracy: 0.5912\n",
      "Epoch 55/500\n",
      "266/266 [==============================] - 175s 656ms/step - loss: 1.2105 - accuracy: 0.4760 - val_loss: 0.9817 - val_accuracy: 0.7336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "266/266 [==============================] - 174s 654ms/step - loss: 1.1321 - accuracy: 0.5085 - val_loss: 0.9248 - val_accuracy: 0.6825\n",
      "Epoch 57/500\n",
      "266/266 [==============================] - 175s 656ms/step - loss: 1.1423 - accuracy: 0.5075 - val_loss: 0.7897 - val_accuracy: 0.7153\n",
      "Epoch 58/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 1.1030 - accuracy: 0.5381 - val_loss: 0.7903 - val_accuracy: 0.6934\n",
      "Epoch 59/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 1.0921 - accuracy: 0.5296 - val_loss: 0.8539 - val_accuracy: 0.6971\n",
      "Epoch 60/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 1.0467 - accuracy: 0.5465 - val_loss: 0.9092 - val_accuracy: 0.7044\n",
      "Epoch 61/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 1.0425 - accuracy: 0.5479 - val_loss: 0.6872 - val_accuracy: 0.7445\n",
      "Epoch 62/500\n",
      "266/266 [==============================] - 174s 656ms/step - loss: 1.0546 - accuracy: 0.5602 - val_loss: 0.7604 - val_accuracy: 0.7372\n",
      "Epoch 63/500\n",
      "266/266 [==============================] - 174s 656ms/step - loss: 1.0303 - accuracy: 0.5503 - val_loss: 0.7624 - val_accuracy: 0.7993\n",
      "Epoch 64/500\n",
      "266/266 [==============================] - 174s 656ms/step - loss: 1.0240 - accuracy: 0.5639 - val_loss: 0.7796 - val_accuracy: 0.7810\n",
      "Epoch 65/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 1.0164 - accuracy: 0.5686 - val_loss: 0.7909 - val_accuracy: 0.7409\n",
      "Epoch 66/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.9978 - accuracy: 0.5611 - val_loss: 0.8441 - val_accuracy: 0.7263\n",
      "Epoch 67/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.9665 - accuracy: 0.5818 - val_loss: 0.7959 - val_accuracy: 0.6934\n",
      "Epoch 68/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.9869 - accuracy: 0.5804 - val_loss: 0.7871 - val_accuracy: 0.7372\n",
      "Epoch 69/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.9767 - accuracy: 0.5841 - val_loss: 0.7579 - val_accuracy: 0.6898\n",
      "Epoch 70/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.9842 - accuracy: 0.5921 - val_loss: 0.7080 - val_accuracy: 0.8066\n",
      "Epoch 71/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.9577 - accuracy: 0.5893 - val_loss: 0.7500 - val_accuracy: 0.7518\n",
      "Epoch 72/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.9279 - accuracy: 0.6114 - val_loss: 0.7032 - val_accuracy: 0.6898\n",
      "Epoch 73/500\n",
      "266/266 [==============================] - 179s 674ms/step - loss: 0.9170 - accuracy: 0.6288 - val_loss: 0.7565 - val_accuracy: 0.7701\n",
      "Epoch 74/500\n",
      "266/266 [==============================] - 174s 656ms/step - loss: 0.9123 - accuracy: 0.6424 - val_loss: 0.6069 - val_accuracy: 0.7847\n",
      "Epoch 75/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.9197 - accuracy: 0.6335 - val_loss: 0.9471 - val_accuracy: 0.5073\n",
      "Epoch 76/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.8793 - accuracy: 0.6645 - val_loss: 0.6077 - val_accuracy: 0.8686\n",
      "Epoch 77/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.8529 - accuracy: 0.6579 - val_loss: 0.5501 - val_accuracy: 0.8650\n",
      "Epoch 78/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.8500 - accuracy: 0.6772 - val_loss: 0.4668 - val_accuracy: 0.9234\n",
      "Epoch 79/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.8468 - accuracy: 0.6748 - val_loss: 0.8240 - val_accuracy: 0.7299\n",
      "Epoch 80/500\n",
      "266/266 [==============================] - 174s 655ms/step - loss: 0.8062 - accuracy: 0.6931 - val_loss: 0.6735 - val_accuracy: 0.7628\n",
      "Epoch 81/500\n",
      "266/266 [==============================] - 176s 661ms/step - loss: 0.7958 - accuracy: 0.6960 - val_loss: 0.5874 - val_accuracy: 0.8139\n",
      "Epoch 82/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.7921 - accuracy: 0.7035 - val_loss: 0.4656 - val_accuracy: 0.9015\n",
      "Epoch 83/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.8067 - accuracy: 0.6936 - val_loss: 0.8697 - val_accuracy: 0.7190\n",
      "Epoch 84/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.7965 - accuracy: 0.6941 - val_loss: 0.5884 - val_accuracy: 0.8869\n",
      "Epoch 85/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.7969 - accuracy: 0.7068 - val_loss: 0.8083 - val_accuracy: 0.7153\n",
      "Epoch 86/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.7746 - accuracy: 0.7162 - val_loss: 0.4735 - val_accuracy: 0.8942\n",
      "Epoch 87/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.7583 - accuracy: 0.7129 - val_loss: 0.6235 - val_accuracy: 0.7920\n",
      "Epoch 88/500\n",
      "266/266 [==============================] - 175s 659ms/step - loss: 0.8108 - accuracy: 0.6997 - val_loss: 0.6807 - val_accuracy: 0.7190\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 89/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.7247 - accuracy: 0.7354 - val_loss: 0.4998 - val_accuracy: 0.9124\n",
      "Epoch 90/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.7101 - accuracy: 0.7397 - val_loss: 0.4763 - val_accuracy: 0.9197\n",
      "Epoch 91/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.7056 - accuracy: 0.7467 - val_loss: 0.5337 - val_accuracy: 0.8869\n",
      "Epoch 92/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.7074 - accuracy: 0.7491 - val_loss: 0.4869 - val_accuracy: 0.9015\n",
      "Epoch 93/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.6725 - accuracy: 0.7594 - val_loss: 0.5963 - val_accuracy: 0.8066\n",
      "Epoch 94/500\n",
      "266/266 [==============================] - 175s 659ms/step - loss: 0.6934 - accuracy: 0.7491 - val_loss: 0.4461 - val_accuracy: 0.9124\n",
      "Epoch 95/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.6877 - accuracy: 0.7495 - val_loss: 0.5246 - val_accuracy: 0.9015\n",
      "Epoch 96/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.7030 - accuracy: 0.7481 - val_loss: 0.4527 - val_accuracy: 0.9270\n",
      "Epoch 97/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.6770 - accuracy: 0.7580 - val_loss: 0.4277 - val_accuracy: 0.9380\n",
      "Epoch 98/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.6710 - accuracy: 0.7697 - val_loss: 0.4481 - val_accuracy: 0.9234\n",
      "Epoch 99/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.6524 - accuracy: 0.7702 - val_loss: 0.4477 - val_accuracy: 0.9307\n",
      "Epoch 100/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.6216 - accuracy: 0.7843 - val_loss: 0.5810 - val_accuracy: 0.9234\n",
      "Epoch 101/500\n",
      "266/266 [==============================] - 175s 656ms/step - loss: 0.6522 - accuracy: 0.7763 - val_loss: 0.5161 - val_accuracy: 0.9161\n",
      "Epoch 102/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.6206 - accuracy: 0.7820 - val_loss: 0.3536 - val_accuracy: 0.9197\n",
      "Epoch 103/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.6302 - accuracy: 0.7735 - val_loss: 0.4188 - val_accuracy: 0.9416\n",
      "Epoch 104/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.6280 - accuracy: 0.7805 - val_loss: 0.3973 - val_accuracy: 0.9489\n",
      "Epoch 105/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.6477 - accuracy: 0.7669 - val_loss: 0.4389 - val_accuracy: 0.9161\n",
      "Epoch 106/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.6142 - accuracy: 0.7834 - val_loss: 0.4023 - val_accuracy: 0.9453\n",
      "Epoch 107/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.5981 - accuracy: 0.8036 - val_loss: 0.4094 - val_accuracy: 0.9489\n",
      "Epoch 108/500\n",
      "266/266 [==============================] - 175s 659ms/step - loss: 0.6080 - accuracy: 0.7923 - val_loss: 0.4587 - val_accuracy: 0.9416\n",
      "Epoch 109/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.6172 - accuracy: 0.7942 - val_loss: 0.4119 - val_accuracy: 0.9234\n",
      "Epoch 110/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.6038 - accuracy: 0.7890 - val_loss: 0.3630 - val_accuracy: 0.9672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.6079 - accuracy: 0.7918 - val_loss: 0.3829 - val_accuracy: 0.9088\n",
      "Epoch 112/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.6001 - accuracy: 0.8040 - val_loss: 0.4421 - val_accuracy: 0.9416\n",
      "Epoch 113/500\n",
      "266/266 [==============================] - 174s 656ms/step - loss: 0.6144 - accuracy: 0.7895 - val_loss: 0.4144 - val_accuracy: 0.9234\n",
      "Epoch 114/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.5953 - accuracy: 0.8036 - val_loss: 0.3641 - val_accuracy: 0.9453\n",
      "Epoch 115/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.6094 - accuracy: 0.7993 - val_loss: 0.3748 - val_accuracy: 0.9562\n",
      "Epoch 116/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.5529 - accuracy: 0.8134 - val_loss: 0.3502 - val_accuracy: 0.9526\n",
      "Epoch 117/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.5944 - accuracy: 0.7942 - val_loss: 0.3212 - val_accuracy: 0.9599\n",
      "Epoch 118/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.5803 - accuracy: 0.8008 - val_loss: 0.4192 - val_accuracy: 0.9161\n",
      "Epoch 119/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.5818 - accuracy: 0.8059 - val_loss: 0.2931 - val_accuracy: 0.9599\n",
      "Epoch 120/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.5386 - accuracy: 0.8271 - val_loss: 0.3338 - val_accuracy: 0.9416\n",
      "\n",
      "Epoch 00120: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 121/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.5232 - accuracy: 0.8360 - val_loss: 0.3709 - val_accuracy: 0.9599\n",
      "Epoch 122/500\n",
      "266/266 [==============================] - 175s 656ms/step - loss: 0.5469 - accuracy: 0.8275 - val_loss: 0.3082 - val_accuracy: 0.9599\n",
      "Epoch 123/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.5088 - accuracy: 0.8360 - val_loss: 0.3628 - val_accuracy: 0.9307\n",
      "Epoch 124/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.5199 - accuracy: 0.8327 - val_loss: 0.3632 - val_accuracy: 0.9489\n",
      "Epoch 125/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.5042 - accuracy: 0.8416 - val_loss: 0.3217 - val_accuracy: 0.9526\n",
      "Epoch 126/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.5228 - accuracy: 0.8322 - val_loss: 0.3002 - val_accuracy: 0.9635\n",
      "Epoch 127/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.5121 - accuracy: 0.8351 - val_loss: 0.3152 - val_accuracy: 0.9599\n",
      "Epoch 128/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.5188 - accuracy: 0.8214 - val_loss: 0.3677 - val_accuracy: 0.9562\n",
      "Epoch 129/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.5006 - accuracy: 0.8374 - val_loss: 0.3309 - val_accuracy: 0.9562\n",
      "Epoch 130/500\n",
      "266/266 [==============================] - 175s 659ms/step - loss: 0.4879 - accuracy: 0.8463 - val_loss: 0.3272 - val_accuracy: 0.9562\n",
      "\n",
      "Epoch 00130: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 131/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.4417 - accuracy: 0.8553 - val_loss: 0.3011 - val_accuracy: 0.9635\n",
      "Epoch 132/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.4951 - accuracy: 0.8407 - val_loss: 0.3339 - val_accuracy: 0.9489\n",
      "Epoch 133/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.5129 - accuracy: 0.8379 - val_loss: 0.3624 - val_accuracy: 0.9380\n",
      "Epoch 134/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.4946 - accuracy: 0.8459 - val_loss: 0.2846 - val_accuracy: 0.9708\n",
      "Epoch 135/500\n",
      "266/266 [==============================] - 175s 660ms/step - loss: 0.4801 - accuracy: 0.8459 - val_loss: 0.3131 - val_accuracy: 0.9635\n",
      "Epoch 136/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.4775 - accuracy: 0.8529 - val_loss: 0.3321 - val_accuracy: 0.9599\n",
      "Epoch 137/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.4533 - accuracy: 0.8623 - val_loss: 0.3344 - val_accuracy: 0.9489\n",
      "Epoch 138/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.4658 - accuracy: 0.8520 - val_loss: 0.3353 - val_accuracy: 0.9562\n",
      "Epoch 139/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.4885 - accuracy: 0.8454 - val_loss: 0.2926 - val_accuracy: 0.9599\n",
      "Epoch 140/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.4641 - accuracy: 0.8539 - val_loss: 0.3305 - val_accuracy: 0.9562\n",
      "Epoch 141/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.4454 - accuracy: 0.8656 - val_loss: 0.2667 - val_accuracy: 0.9635\n",
      "Epoch 142/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.4474 - accuracy: 0.8604 - val_loss: 0.3107 - val_accuracy: 0.9708\n",
      "Epoch 143/500\n",
      "266/266 [==============================] - 175s 656ms/step - loss: 0.4542 - accuracy: 0.8618 - val_loss: 0.3211 - val_accuracy: 0.9672\n",
      "Epoch 144/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.4517 - accuracy: 0.8618 - val_loss: 0.3148 - val_accuracy: 0.9562\n",
      "\n",
      "Epoch 00144: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 145/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.4287 - accuracy: 0.8661 - val_loss: 0.3082 - val_accuracy: 0.9562\n",
      "Epoch 146/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.4332 - accuracy: 0.8600 - val_loss: 0.3025 - val_accuracy: 0.9599\n",
      "Epoch 147/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.4188 - accuracy: 0.8750 - val_loss: 0.3056 - val_accuracy: 0.9635\n",
      "Epoch 148/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.4139 - accuracy: 0.8750 - val_loss: 0.2912 - val_accuracy: 0.9599\n",
      "Epoch 149/500\n",
      "266/266 [==============================] - 175s 656ms/step - loss: 0.4475 - accuracy: 0.8557 - val_loss: 0.2970 - val_accuracy: 0.9599\n",
      "Epoch 150/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.4333 - accuracy: 0.8675 - val_loss: 0.2713 - val_accuracy: 0.9562\n",
      "Epoch 151/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.4350 - accuracy: 0.8684 - val_loss: 0.2988 - val_accuracy: 0.9599\n",
      "Epoch 152/500\n",
      "266/266 [==============================] - 175s 656ms/step - loss: 0.4405 - accuracy: 0.8665 - val_loss: 0.2925 - val_accuracy: 0.9599\n",
      "Epoch 153/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.4011 - accuracy: 0.8806 - val_loss: 0.2557 - val_accuracy: 0.9672\n",
      "Epoch 154/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.4340 - accuracy: 0.8778 - val_loss: 0.2687 - val_accuracy: 0.9672\n",
      "\n",
      "Epoch 00154: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 155/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.4091 - accuracy: 0.8788 - val_loss: 0.2882 - val_accuracy: 0.9635\n",
      "Epoch 156/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.3991 - accuracy: 0.8844 - val_loss: 0.2765 - val_accuracy: 0.9672\n",
      "Epoch 157/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.4172 - accuracy: 0.8769 - val_loss: 0.2693 - val_accuracy: 0.9635\n",
      "Epoch 158/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.4179 - accuracy: 0.8750 - val_loss: 0.2681 - val_accuracy: 0.9672\n",
      "Epoch 159/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.4132 - accuracy: 0.8783 - val_loss: 0.2684 - val_accuracy: 0.9672\n",
      "Epoch 160/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.4051 - accuracy: 0.8839 - val_loss: 0.2926 - val_accuracy: 0.9599\n",
      "Epoch 161/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.4271 - accuracy: 0.8717 - val_loss: 0.3105 - val_accuracy: 0.9599\n",
      "Epoch 162/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.4003 - accuracy: 0.8825 - val_loss: 0.2782 - val_accuracy: 0.9635\n",
      "Epoch 163/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.4440 - accuracy: 0.8717 - val_loss: 0.2942 - val_accuracy: 0.9599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.4149 - accuracy: 0.8778 - val_loss: 0.2842 - val_accuracy: 0.9489\n",
      "\n",
      "Epoch 00164: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 165/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.3968 - accuracy: 0.8820 - val_loss: 0.2807 - val_accuracy: 0.9562\n",
      "Epoch 166/500\n",
      "266/266 [==============================] - 175s 659ms/step - loss: 0.4004 - accuracy: 0.8825 - val_loss: 0.2769 - val_accuracy: 0.9599\n",
      "Epoch 167/500\n",
      "266/266 [==============================] - 175s 656ms/step - loss: 0.4240 - accuracy: 0.8670 - val_loss: 0.2839 - val_accuracy: 0.9489\n",
      "Epoch 168/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.4284 - accuracy: 0.8633 - val_loss: 0.2864 - val_accuracy: 0.9562\n",
      "Epoch 169/500\n",
      "266/266 [==============================] - 175s 657ms/step - loss: 0.3963 - accuracy: 0.8858 - val_loss: 0.3107 - val_accuracy: 0.9526\n",
      "Epoch 170/500\n",
      "266/266 [==============================] - 175s 656ms/step - loss: 0.4252 - accuracy: 0.8773 - val_loss: 0.2772 - val_accuracy: 0.9562\n",
      "Epoch 171/500\n",
      "266/266 [==============================] - 175s 659ms/step - loss: 0.4031 - accuracy: 0.8722 - val_loss: 0.3005 - val_accuracy: 0.9562\n",
      "Epoch 172/500\n",
      "266/266 [==============================] - 178s 668ms/step - loss: 0.3959 - accuracy: 0.8849 - val_loss: 0.2861 - val_accuracy: 0.9526\n",
      "Epoch 173/500\n",
      "266/266 [==============================] - 175s 658ms/step - loss: 0.4262 - accuracy: 0.8717 - val_loss: 0.2845 - val_accuracy: 0.9562\n"
     ]
    }
   ],
   "source": [
    "history = model_d121.fit_generator(datagen.flow(x_train, y_train, batch_size=8),\n",
    "                             epochs=500,\n",
    "                             steps_per_epoch=x_train.shape[0]//8,\n",
    "                             verbose=1,\n",
    "                             callbacks=[es, lr],\n",
    "                             validation_data=(x_val, y_val)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_train\n",
    "del train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "test_ids = test_df[\"image_id\"]\n",
    "test_image = []\n",
    "for name in test_df[\"image_id\"]:\n",
    "    image = Image.open(image_path+name+\".jpg\")\n",
    "    image = image.resize((img_size, img_size))\n",
    "    test_image.append(image)\n",
    "x_test = np.ndarray(shape=(len(test_image), img_size, img_size, 3), dtype=np.float32)\n",
    "for i in range(len(test_image)):\n",
    "    x_test[i] = img_to_array(test_image[i])\n",
    "x_test /= 255\n",
    "del test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_d121.predict(x_test)\n",
    "res = pd.DataFrame()\n",
    "res[\"image_id\"]=test_ids\n",
    "res[\"healthy\"] = pred[:, 0]\n",
    "res[\"multiple_diseases\"] = pred[:, 1]\n",
    "res[\"rust\"] = pred[:, 2]\n",
    "res[\"scab\"] = pred[:, 3]\n",
    "res.to_csv(\"submission_simple_resnet50_400.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "h = history.history\n",
    "offset = 5\n",
    "epochs = range(offset, len(h[\"loss\"]))\n",
    "plt.figure(1, figsize=(20,20))\n",
    "plt.subplot(121)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(epochs, h[\"loss\"][offset:], label=\"train\")\n",
    "plt.plot(epochs, h[\"val_loss\"][offset:], label=\"val\")\n",
    "plt.legend()\n",
    "plt.subplot(122)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.plot(h[f'accuracy'], label='train')\n",
    "plt.plot(h[f'val_accuracy'], label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
